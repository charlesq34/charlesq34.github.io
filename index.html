<head>
<title>Charles R. Qi</title>
<link rel="stylesheet" type="text/css" href="resources/main.css">
</head>

<body>

<div id="dhead" class="container">
  <div class="row">
    <div id="dpic">
      <img src="resources/rqi_2022.png" class="ppic">
    </div>
    <div id="ddesc">
      <h1>Charles R. Qi</h1>
      <h2>Machine Learning Engineer, Tesla</h2>
      <!--
      <div style="font-size:26; font-weight: 400; margin-bottom: 10px">Charles Ruizhongtai Qi</div>
      <div style="font-weight: 300; margin-bottom: 15px">Staff Research Scientist, Waymo LLC</div>
      <b style="font-weight: 450">Email:</b> rqi [at] stanford [dot] edu<br>
      -->
      <div id="dico">
        <div style="margin-top: 15px; padding: 0px">
        <a href="#publications">[Publications]</a>&nbsp;
        <a href="#talks">[Talks]</a>&nbsp;
        <a href="misc.html">[Misc]</a>&nbsp;
	<a href="blog/index.html">[Blog]</a>&nbsp;
	</div>
        <div style="margin-top: 3px; padding: 0px">
        <a href="https://scholar.google.com/citations?user=4jODkxsAAAAJ&hl=en" target="_blank">[Google Scholar]</a>&nbsp;
        <a href="https://github.com/charlesq34" target="_blank">[GitHub]</a>&nbsp;
        <a href="https://www.linkedin.com/in/charles-ruizhongtai-qi-53a43739" target="_blank">[LinkedIn]</a>
	</div>
      </div>
    </div>
  </div>
</div>

<hr>


<script type="text/javascript">
function hideshow(which){
if (!document.getElementById)
return
if (which.style.display=="block")
which.style.display="none"
else
which.style.display="block"
}
</script>

<div class="section">
<p>
I am currently a Machine Learning Engineer at Tesla Autopilot, working on FSD (2024-Present). Before I joined Tesla in 2024/3, I was a Research Scientist and Manager at <a href="http://waymo.com" target="_blank">Waymo</a> (2019-2024), leading a team to build the perception foundation models for the next-gen autonomy system. At Waymo, I also worked on 3D perception, auto labeling and data-driven simulation, leading to 15+ publications at top-tier conferences and multiple successful launches.
<br><br>
In 2018-2019, I was a Postdoctoral Researcher at Facebook AI Research (FAIR) where I was fortunate to have worked with stellar researchers like <a href="https://kaiminghe.github.io/">Kaiming He</a>, <a href="https://www.sainingxie.com/">Saining Xie</a> and <a href="https://xinleic.xyz/">Xinlei Chen</a> on several projects about 3D perception (<a href="https://github.com/facebookresearch/votenet">VoteNet</a>, <a href="https://arxiv.org/abs/2001.10692">ImVoteNet</a>, <a href="https://arxiv.org/abs/2007.10985">PointContrast</a>). Before that, I spent five memorable years (2013-2018) at <a href="http://stanford.edu" target="_blank">Stanford University</a> obtaining my Ph.D., at <a href="http://ai.stanford.edu" target="_blank">Stanford AI Lab</a> and <a href="https://geometry.stanford.edu" target="_blank">Geometric Computation Group</a>, advised by Professor <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a>. During my Ph.D. we had a series of work that started the field of 3D deep learning (<a href="https://stanford.edu/~rqi/pointnet/">PointNet</a>, <a href="https://stanford.edu/~rqi/pointnet2/">PointNet++</a> and <a href="https://graphics.stanford.edu/projects/3dcnn/">Multi-View/Volumetric CNNs</a>) and explored their various applications for 3D scene understanding (<a href="https://github.com/charlesq34/frustum-pointnets">detection</a>, <a href="https://arxiv.org/abs/1904.08889">segmentation</a>, <a href="https://arxiv.org/abs/1806.01411">scene flow</a>, <a href="https://graphics.stanford.edu/projects/cnncomplete/">shape synthesis</a>). Such models have been widely used in both academia and industry (autonomous driving, augmented reality and robotics). Prior to joining Stanford, I got my B.Eng. from <a href="http://www.tsinghua.edu.cn" target="_blank">Tsinghua University</a>. <br/><br/>

You can follow me on <a href="https://twitter.com/charles_rqi?lang=en" target="_blank">Twitter/X</a> (@charles_rqi) for more updates!
</p>
<!--<br>
My research focuses on deep learning, computer vision and 3D. I have developed novel deep learning architectures for 3D data (point clouds, volumetric grids and multi-view images) that have wide applications in 3D object classification, object part segmentation, semantic scene parsing, scene flow estimation and 3D reconstruction. Those deep architectures have been well adopted by both academic and industrial groups across the world. I have also invented several state-of-the-art methods for 3D object recognition, which reinforce current and future applications in augmented reality and robotics. At Waymo I developed scalable and data-efficient perception and data-driven simulation for both onboard and offboard use cases. My recent interest is on connecting foundation models to embodied AI and perception. If you are interested in my research or have any use cases to share, feel free to contact me!-->
</p>
</div>




<a name="publications"></a>
<div class="mainsection">
<div style="font-weight: 500; font-size: 24px; margin-bottom: 10px">Publications</div>

<!-- VLM -->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px">
<tr>
<td width="25%" valign="center"><p><img src="papers/vlm.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="center"><p>
<div class="paper-title"><a href="https://arxiv.org/pdf/2309.14491">Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving</a></div>
<div style="margin-bottom: 10px; padding: 0px">Mahyar Najibi, Jingwei Ji, Yin Zhou, <b>Charles R. Qi</b>, Xinchen Yan, Scott Ettinger, Dragomir Anguelov</div>
<div style="margin-bottom: 10px; padding: 0px"><i>ICCV 2023</i></div>
<a href="https://arxiv.org/pdf/2309.14491.pdf">paper</a> / <a href="javascript:hideshow(document.getElementById('VLM'))">bibtex</a>
<br>
<p id="VLM" style="font:16px; display: none; max-width=500px">
@inproceedings{najibi2023unsupervised,
  title={Unsupervised 3d perception with 2d vision-language distillation for autonomous driving},
  author={Najibi, Mahyar and Ji, Jingwei and Zhou, Yin and Qi, Charles R and Yan, Xinchen and Ettinger, Scott and Anguelov, Dragomir},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8602--8612},
  year={2023}
}
</table>
</div> 

<!-- MODAR -->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px">
<tr>
    <td width="25%" valign="center"><p><img src="papers/modar.png" width="250" alt="" style="border-style: none" align="top"></p></td>
    <td width="75%" valign="center">
        <p>
            <div class="paper-title"><a href="https://arxiv.org/pdf/2306.03206">MoDAR: Using Motion Forecasting for 3D Object Detection in Point Cloud Sequences</a></div>
            <div style="margin-bottom: 10px; padding: 0px">Yingwei Li*, <b>Charles R. Qi</b>*, Yin Zhou, Chenxi Liu, Dragomir Anguelov (*: equal contribution)</div>
            <div style="margin-bottom: 10px; padding: 0px"><i>CVPR 2023</i></div>
            <a href="https://arxiv.org/pdf/2306.03206.pdf">paper</a> / <a href="javascript:hideshow(document.getElementById('MODAR'))">bibtex</a>
            <br>
            <p id="MODAR" style="font:16px; display: none; max-width=500px">
                @inproceedings{li2023modar,
                  title={MoDAR: Using Motion Forecasting for 3D Object Detection in Point Cloud Sequences},
                  author={Li, Yingwei and Qi, Charles R and Zhou, Yin and Liu, Chenxi and Anguelov, Dragomir},
                  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
                  pages={9329--9339},
                  year={2023}
                }
            </p>
        </p>
    </td>
</tr>
</table>
</div>

<!-- GINA -->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
    <td width="25%" valign="center"><p><img src="papers/gina.png" width="250" alt="" style="border-style: none"></p></td>
    <td width="75%" valign="center"><p>
	    <div class="paper-title"><a href="https://arxiv.org/abs/2304.02163">GINA-3D: Learning to Generate Implicit Neural Assets in the Wild</a></div>
	    <div style="margin-bottom: 10px; padding: 0px">Bokui Shen, Xinchen Yan, <b>Charles R. Qi</b>, Mahyar Najibi, Boyang Deng, Leonidas Guibas, Yin Zhou, Dragomir Anguelov</div>
            <div style="margin-bottom: 10px; padding: 0px"><i>CVPR 2023</i></div>
	    <a href="https://arxiv.org/pdf/2304.02163.pdf">paper</a> / <a href="javascript:hideshow(document.getElementById('GINA'))">bibtex</a> / <a href="https://waymo.com/open/data/perception/#object-assets">dataset</a>
<br>
<p id="GINA" style="font:18px; display: none">
@inproceedings{shen2023gina,
  title={GINA-3D: Learning to Generate Implicit Neural Assets in the Wild},
  author={Shen, Bokui and Yan, Xinchen and Qi, Charles R and Najibi, Mahyar and Deng, Boyang and Guibas, Leonidas and Zhou, Yin and Anguelov, Dragomir},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4913--4926},
  year={2023}
}
</table>
</div>

<!-- NERDI -->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
<td width="25%" valign="center"><p><img src="papers/nerdi.png" width="250" alt="" style="border-style: none"></p></td>
<td width="75%" valign="center"><p>
	<div class="paper-title"><a href="https://arxiv.org/abs/2212.03267">NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors</a></div>
	<div style="margin-bottom: 10px; padding: 0px">Congyue Deng, Chiyu Max Jiang, <b>Charles R. Qi</b>, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov</div>
	<div style="margin-bottom: 10px; padding: 0px"><i>CVPR 2023</i></div>
	<a href="https://arxiv.org/pdf/2212.03267.pdf">paper</a> / <a href="javascript:hideshow(document.getElementById('NERDI'))">bibtex</a>
<br>
<p id="NERDI" style="font:18px; display: none">
@inproceedings{deng2023nerdi,
  title={Nerdi: Single-view nerf synthesis with language-guided diffusion as general image priors},
  author={Deng, Congyue and Jiang, Chiyu and Qi, Charles R and Yan, Xinchen and Zhou, Yin and Guibas, Leonidas and Anguelov, Dragomir and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20637--20647},
  year={2023}
}
</table>
</div>

<!-- LESS -->
<div class="rounded-rectangle" style="margin-bottom:15px;">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
	<td width="25%" valign="center"><p><img src="papers/less.png" width="250" alt="" style="border-style: none"></p></td>
	<td width="75%" valign="center"><p>
		<div class="paper-title"><a href="https://arxiv.org/abs/2210.08064">LESS: Label-Efficient Semantic Segmentation for LiDAR Point Clouds</a></b></div>
		<div style="margin-bottom: 10px; padding: 0px">Minghua Liu, Yin Zhou, <b>Charles R. Qi</b>, Boqing Gong, Hao Su, Dragomir Anguelov</div>
		<div style="margin-bottom: 10px; padding: 0px"><i>ECCV 2022, <font color="#d22d1d">Oral Presentation</font></i></div>
		<a href="https://arxiv.org/pdf/2210.08064.pdf">paper</a> / <a href="javascript:hideshow(document.getElementById('LESS'))">bibtex</a>
<br>
<p id="LESS" style="font:18px; display: none">
@article{liu2022less,
  title={LESS: Label-Efficient Semantic Segmentation for LiDAR Point Clouds},
  author={Liu, Minghua and Zhou, Yin and Qi, Charles R and Gong, Boqing and Su, Hao and Anguelov, Dragomir},
  journal={arXiv preprint arXiv:2210.08064},
  year={2022}
}
</tr>
</table>
</div>

<!-- REM -->
<div class="rounded-rectangle" style="margin-bottom:15px;">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
	<td width="25%" valign="center"><p><img src="papers/rem.png" width="250" alt="" style="border-style: none" align="top"></p></td>
	<td width="75%" valign="center"><p>
		<div class="paper-title"><a href="https://arxiv.org/abs/2210.08375">Improving the Intra-class Long-tail in 3D Detection via Rare Example Mining</a></div>
		<div style="margin-bottom: 10px; padding: 0px">Chiyu (Max) Jiang, Mahyar Najibi, <b>Charles R. Qi</b>, Yin Zhou, Dragomir Anguelov</div>
		<div style="margin-bottom: 10px; padding: 0px"><i>ECCV 2022</i></div>
<a href="https://arxiv.org/pdf/2210.08375.pdf">paper</a> / <a href="javascript:hideshow(document.getElementById('REM'))">bibtex</a>
<br>
<p id="REM" style="font:18px; display: none">
@article{jiang2022improving,
  title={Improving the Intra-class Long-tail in 3D Detection via Rare Example Mining},
  author={Jiang, Chiyu Max and Najibi, Mahyar and Qi, Charles R and Zhou, Yin and Anguelov, Dragomir},
  journal={arXiv preprint arXiv:2210.08375},
  year={2022}
}
</tr>
</table>
</div>

<!-- Lidar RIDDLE -->
<div class="rounded-rectangle" style="margin-bottom:15px;">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
	<td width="25%" valign="center"><p><img src="papers/riddle.png" width="250" alt="" style="border-style: none"></p></td>
	<td width="75%" valign="center"><p>
		<div class="paper-title"><a href="https://arxiv.org/pdf/2206.01738.pdf">RIDDLE: Lidar Data Compression with Range Image Deep Delta Encoding</a></div>
		<div style="margin-bottom: 10px; padding: 0px">Xuanyu Zhou*, <b>Charles R. Qi</b>*, Yin Zhou, Dragomir Anguelov (*: equal contribution)</div>
		<div style="margin-bottom: 10px; padding: 0px"><i>CVPR 2022</i></div>
<a href="https://arxiv.org/pdf/2206.01738.pdf">paper</a> / <a href="javascript:hideshow(document.getElementById('RIDDLE'))">bibtex</a>
<br>
<p id="RIDDLE" style="font:18px; display: none">
@inproceedings{zhou2022riddle,
  title={RIDDLE: Lidar Data Compression with Range Image Deep Delta Encoding},
  author={Zhou, Xuanyu and Qi, Charles R and Yin, Zhou and Anguelov, Dragomir},
  journal={Proc. Computer Vision and Pattern Recognition (CVPR), IEEE},
  year={2022}
}
</tr>
</table>
</div>

<!-- Egocentric -->
<div class="rounded-rectangle" style="margin-bottom:15px;">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
	<td width="25%" valign="center"><p><img src="papers/egocentric.png" width="250" alt="" style="border-style: none"></p></td>
	<td width="75%" valign="center"><p>
		<div class="paper-title"><a href="https://openreview.net/pdf?id=OMNRFw1fX3a">Revisiting 3D Object Detection From an Egocentric Perspective</a></div>
		<div style="margin-bottom: 10px; padding: 0px">Boyang Deng, <b>Charles R. Qi</b>, Mahyar Najibi, Thomas Funkhouser, Yin Zhou, Dragomir Anguelov</div>
		<div style="margin-bottom: 10px; padding: 0px"><i>Neurips 2021</i></div>
<a href="https://openreview.net/pdf?id=OMNRFw1fX3a">paper</a> / <a href="javascript:hideshow(document.getElementById('EGOCENTRIC'))">bibtex</a>
<br>
<p id="EGOCENTRIC" style="font:18px; display: none">
@inproceedings{deng2021revisiting,
  title={Revisiting 3D Object Detection From an Egocentric Perspective},
  author={Deng, Boyang and Qi, Charles R and Najibi, Mahyar and Funkhouser, Thomas and Zhou, Yin and Anguelov, Dragomir},
  booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
  year={2021}
}
</tr>
</table>
</div>

<!-- SPG -->
<div class="rounded-rectangle" style="margin-bottom:15px;">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
	<td width="25%" valign="center"><p><img src="papers/spg.png" width="250" alt="" style="border-style: none"></p></td>
	<td width="75%" valign="center"><p>
<div class="paper-title"><a href="https://arxiv.org/abs/2108.06709">SPG: Unsupervised Domain Adaptation for 3D Object Detection via Semantic Point Generation</a></div>
<div style="margin-bottom: 10px; padding: 0px">Qiangeng Xu, Yin Zhou, Weiyue Wang, <b>Charles R. Qi</b>, Dragomir Anguelov</div>
<div style="margin-bottom: 10px; padding: 0px"><i>ICCV 2021</i></div>
<a href="https://arxiv.org/pdf/2108.06709.pdf">paper</a> / <a href="javascript:hideshow(document.getElementById('SPG'))">bibtex</a>
<br>
<p id="SPG" style="font:18px; display: none">
@inproceedings{xu2021spg,
  title={SPG: Unsupervised Domain Adaptation for 3D Object Detection via Semantic Point Generation},
  author={Xu, Qiangeng and Zhou, Yin and Wang, Weiyue and Qi, Charles R and Anguelov, Dragomir},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15446--15456},
  year={2021}
}
</tr>
</table>
</div>

<!-- Motion Dataset -->
<div class="rounded-rectangle" style="margin-bottom:15px;">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
	<td width="25%" valign="center"><p><img src="papers/motion_dataset.png" width="250" alt="" style="border-style: none"></p></td>
	<td width="75%" valign="center"><p>
<div class="paper-title"><a href="https://arxiv.org/abs/2104.10133">Large Scale Interactive Motion Forecasting for Autonomous Driving : The Waymo Open Motion Dataset</a></div>
<div style="margin-bottom: 10px; padding: 0px">S. Ettinger, S. Cheng, B. Caine, C. Liu, H. Zhao, S. Pradhan, Y. Chai, B. Sapp, <b>Charles R. Qi</b>, Y. Zhou, Z. Yang, A. Chouard, P. Sun, J. Ngiam, V. Vasudevan, A. McCauley, J. Shlens, D. Anguelov</div>
<div style="margin-bottom: 10px; padding: 0px"><i>ICCV 2021</i></div>
<a href="https://arxiv.org/pdf/2104.10133.pdf">paper</a> / <a href="https://waymo.com/open/data/motion/">dataset</a> / <a href="javascript:hideshow(document.getElementById('MOTION_DATASET'))">bibtex</a>
<br>
<p id="MOTION_DATASET" style="font:18px; display: none">
@article{ettinger2021large,
  title={Large Scale Interactive Motion Forecasting for Autonomous Driving: The Waymo Open Motion Dataset},
  author={Ettinger, Scott and Cheng, Shuyang and Caine, Benjamin and Liu, Chenxi and Zhao, Hang and Pradhan, Sabeek and Chai, Yuning and Sapp, Ben and Qi, Charles and Zhou, Yin and others},
  journal={arXiv preprint arXiv:2104.10133},
  year={2021}
}
</tr>
</table>
</div>

<!-- Offboard Detection -->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
	<td width="25%" valign="center"><p><img src="papers/offboard_detection.png" width="250" alt="" style="border-style: none"></p></td>
	<td width="75%" valign="center"><p>
<div class="paper-title"><a href="https://arxiv.org/abs/2103.05073">Offboard 3D Object Detection from Point Cloud Sequences</a></div>
<div style="margin-bottom: 10px; padding: 0px"><b>Charles R. Qi</b>, Yin Zhou, Mahyar Najibi, Pei Sun, Khoa Vo, Boyang Deng, Dragomir Anguelov</div>
<div style="margin-bottom: 10px; padding: 0px"><i>CVPR 2021</i></div>
<a href="https://arxiv.org/pdf/2103.05073.pdf">paper</a> / <a href="https://blog.waymo.com/2021/03/expanding-waymo-open-dataset-with-interactive-scenario-data-and-new-challenges.html">blog post</a> / <a href="javascript:hideshow(document.getElementById('OFFBOARD_DETECTION'))">bibtex</a> / <a href="https://www.youtube.com/watch?v=hyTMyZgWcFY">talk</a>
<p id="OFFBOARD_DETECTION" style="font:18px; display: none">
@article{qi2021offboard,
  title={Offboard 3D Object Detection from Point Cloud Sequences},
  author={Qi, Charles R and Zhou, Yin and Najibi, Mahyar and Sun, Pei and Vo, Khoa and Deng, Boyang and Anguelov, Dragomir},
  journal={arXiv preprint arXiv:2103.05073},
  year={2021}
}
</tr>
</table>
</div>

<!-- PointContrast -->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
<td width="25%" valign="center"><p><img src="papers/pointcontrast.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="center"><p>
	<div class="paper-title"><a href="https://arxiv.org/pdf/2007.10985">PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding</a></div>
	<div style="margin-bottom: 10px; padding: 0px">Saining Xie, Jiatao Gu, Demi Guo, <b>Charles R. Qi</b>, Leonidas J. Guibas, Or Litany</div>
	<div style="margin-bottom: 10px; padding: 0px"><i>ECCV 2020</i>, <font color="#d22d1d">Spotlight</font></div>
<a href="https://arxiv.org/pdf/2007.10985.pdf">paper</a> / <a href="https://github.com/facebookresearch/PointContrast">code</a> / <a href="javascript:hideshow(document.getElementById('POINTCONTRAST'))">bibtex</a>
<p id="POINTCONTRAST" style="font:18px; display: none">
@article{xie2020pointcontrast,
  title={PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding},
  author={Xie, Saining and Gu, Jiatao and Guo, Demi and Qi, Charles R and Guibas, Leonidas J and Litany, Or},
  journal={arXiv preprint arXiv:2007.10985},
  year={2020}
}
</tr>
</table>
</div>


<!-- Image VoteNet -->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
<td width="25%" valign="center"><p><img src="papers/imvotenet.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="center"><p>
<div class="paper-title"><a href="https://arxiv.org/abs/2001.10692">ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes</a></div>
<div style="margin-bottom: 10px; padding: 0px"><b>Charles R. Qi</b><sup>*</sup>, Xinlei Chen<sup>*</sup>, Or Litany, Leonidas J. Guibas (<sup>*</sup>: equal contribution)</div>
<div style="margin-bottom: 10px; padding: 0px"><i>CVPR 2020</i></div>
<a href="https://arxiv.org/pdf/2001.10692.pdf">paper</a> / <a href="javascript:hideshow(document.getElementById('IMVOTENET'))">bibtex</a> / <a href="https://github.com/facebookresearch/imvotenet">code</a>
<p id="IMVOTENET" style="font:18px; display: none">
@article{qi2020imvotenet,
  title={ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes},
  author={Qi, Charles R and Chen, Xinlei and Litany, Or and Guibas, Leonidas J},
  journal={arXiv preprint arXiv:2001.10692},
  year={2020}
}
</tr>
</table>
</div>


<!-- Deep Hough Voting -->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
<td width="25%" valign="center"><p><img src="papers/votenet.jpg" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="center"><p>
<div class="paper-title"><a href="https://arxiv.org/abs/1904.09664">Deep Hough Voting for 3D Object Detection in Point Clouds</a></div>
<div style="margin-bottom: 10px; padding: 0px"><b>Charles R. Qi</b>, Or Litany, Kaiming He, Leonidas J. Guibas</div>
<div style="margin-bottom: 10px; padding: 0px"><i>ICCV 2019</i>, <font color="#d22d1d">Oral Presentation</font>, <font color="#d22d1d"></div>

<p><b style="font-weight: 500">Best Paper Award Nomination</b></font> (one of the seven among 1,075 accepted papers) [<a href="http://iccv2019.thecvf.com/program/main_conference">link</a>]</p>
<a href="https://arxiv.org/abs/1904.09664.pdf">paper</a> / <a href="javascript:hideshow(document.getElementById('VOTENET'))">bibtex</a> / <a href="https://github.com/facebookresearch/votenet">code</a> / <a href="https://youtu.be/2ntDYowHbZs?t=4585">talk</a>
<p id="VOTENET" style="font:18px; display: none">
@article{qi2019deep,
  title={Deep Hough Voting for 3D Object Detection in Point Clouds},
  author={Qi, Charles R and Litany, Or and He, Kaiming and Guibas, Leonidas J},
  journal={arXiv preprint arXiv:1904.09664},
  year={2019}
}
</tr>
</table>
</div>

<!-- KPConv -->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
<td width="25%" valign="center"><p><img src="papers/kpconv.png" width="250" height="130" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="center"><p>
	<div class="paper-title"><a href="https://arxiv.org/abs/1904.08889">KPConv: Flexible and Deformable Convolution for Point Clouds</a></div>
	<div style="margin-bottom: 10px; padding: 0px">Hugues Thomas, <b>Charles R. Qi</b>, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Francois Goulette, Leonidas J. Guibas</div>
	<div style="margin-bottom: 10px; padding: 0px"><i>ICCV 2019</i></div>
<a href="https://arxiv.org/abs/1904.08889.pdf">paper</a> / <a href="javascript:hideshow(document.getElementById('KPCONV'))">bibtex</a> / <a href="https://github.com/HuguesTHOMAS/KPConv">code</a>
<p id="KPCONV" style="font:18px; display: none">
@article{thomas2019kpconv,
  title={KPConv: Flexible and Deformable Convolution for Point Clouds},
  author={Thomas, Hugues and Qi, Charles R, Deschaud, Jean-Emmanuel and Marcotegui, Beatriz and Goulette, Francois and Guibas, Leonidas J},
  journal={arXiv preprint arXiv:1904.08889},
  year={2019}
}
</tr>
</table>
</div>

<!-- 3D ADV -->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
<td width="25%" valign="center"><p><img src="papers/3d_adv.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="center"><p>
	<div class="paper-title"><a href="https://arxiv.org/abs/1809.07016">Generating 3D Adversarial Point Clouds</a></div>
	<div style="margin-bottom: 10px; padding: 0px">Chong Xiang, <b>Charles R. Qi</b>, Bo Li</div>
	<div style="margin-bottom: 10px; padding: 0px"><i>CVPR 2019</i></div>
<a href="https://arxiv.org/abs/1809.07016.pdf">paper</a> / <a href="javascript:hideshow(document.getElementById('3DADV'))">bibtex</a> / <a href="https://github.com/xiangchong1/3d-adv-pc">code</a>
<p id="3DADV" style="font:18px; display: none">
@article{xiang2019adv,
  title={Generating 3D Adversarial Point Clouds},
  author={Xiang, Chong and and Qi, Charles R and Li, Bo},
  journal={Proc. Computer Vision and Pattern Recognition (CVPR), IEEE},
  year={2019}
}
</tr>
</table>
</div>


<!-- FLOWNET3D -->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
<td width="25%" valign="center"><p><img src="papers/flownet3d.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="center"><p>
	<div class="paper-title"><a href="https://arxiv.org/abs/1806.01411">FlowNet3D: Learning Scene Flow in 3D Point Clouds</a></div>
	<div style="margin-bottom: 10px; padding: 0px">Xingyu Liu<sup>*</sup>, <b>Charles R. Qi</b><sup>*</sup>, Leonidas Guibas (<sup>*</sup>: equal contribution)</div>
	<div style="margin-bottom: 10px; padding: 0px"><i>CVPR 2019</i></div>
<a href="https://arxiv.org/abs/1806.01411.pdf">paper</a> / <a href="javascript:hideshow(document.getElementById('flownet3d'))">bibtex</a> / <a href="https://github.com/xingyul/flownet3d">code</a>
<p id="flownet3d" style="font:18px; display: none">
@article{liu2019flownet3d,
  title={FlowNet3D: Learning Scene Flow in 3D Point Clouds},
  author={Liu, Xingyu and and Qi, Charles R and Guibas, Leonidas J},
  journal={Proc. Computer Vision and Pattern Recognition (CVPR), IEEE},
  year={2019}
}
</tr>
</table>
</div>

<!-- FRUSTUM POINTNETS-->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
<td width="25%" valign="center"><p><img src="papers/frustum_pointnets.jpg" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="center"><p>
<div class="paper-title"><a href="https://arxiv.org/abs/1711.08488">Frustum PointNets for 3D Object Detection from RGB-D Data</a></div>
<div style="margin-bottom: 10px; padding: 0px"><b>Charles R. Qi</b>, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J. Guibas</div>
<div style="margin-bottom: 10px; padding: 0px"><i>CVPR 2018</i></div>
<p>Our method is simple, efficient and effective, ranking at <i>first place</i> for <a href="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d">KITTI 3D object detection benchmark</a> on all categories (11/27/2017).</p>
<a href="https://arxiv.org/abs/1711.08488">paper</a> / <a href="javascript:hideshow(document.getElementById('FrustumPointNets'))">bibtex</a> / <a href="http://github.com/charlesq34/frustum-pointnets">code</a> / <a href="http://stanford.edu/~rqi/frustum-pointnets/">website</a>
<p id="FrustumPointNets" style="font:18px; display: none">
@article{qi2017frustum,
  title={Frustum PointNets for 3D Object Detection from RGB-D Data},
  author={Qi, Charles R and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J},
  journal={Proc. Computer Vision and Pattern Recognition (CVPR), IEEE},
  year={2018}
}
</tr>
</table>
</div>

<!-- POINTNET2-->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
<td width="25%" valign="center"><p><img src="papers/pointnet2.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="center"><p>
	<div class="paper-title"><a href="https://arxiv.org/abs/1706.02413">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a></div>
	<div style="margin-bottom: 10px; padding: 0px"><b>Charles R. Qi</b>, Li Yi, Hao Su, and Leonidas J. Guibas</div>
	<div style="margin-bottom: 10px; padding: 0px"><i>NIPS 2017</i></div>
<a href="https://arxiv.org/abs/1706.02413">paper</a> / <a href="javascript:hideshow(document.getElementById('PointNet2'))">bibtex</a> / <a href="https://github.com/charlesq34/pointnet2">code</a> / <a href="http://stanford.edu/~rqi/pointnet2/">website</a> / <a href="papers/pointnet2_poster.pdf">poster</a>
<p id="PointNet2" style="font:18px; display: none">
@article{qi2017pointnetplusplus,
  title={PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},
  author={Qi, Charles R and Yi, Li and Su, Hao and Guibas, Leonidas J},
  journal={arXiv preprint arXiv:1706.02413},
  year={2017}
}
</tr>
</table>
</div>

<!-- POINTNET -->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
<td width="25%" valign="center"><p><img src="papers/pointnet.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="center"><p>
<div class="paper-title"><a href="https://arxiv.org/abs/1612.00593">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</a></div>
<div style="margin-bottom: 10px; padding: 0px"><b>Charles R. Qi</b><sup>*</sup>, Hao Su<sup>*</sup>, Kaichun Mo, and Leonidas J. Guibas (<sup>*</sup>: equal contribution)</div>
<div style="margin-bottom: 10px; padding: 0px"><i>CVPR 2017</i>, <font color="#d22d1d">Oral Presentation</font></div>
<a href="https://arxiv.org/abs/1612.00593">paper</a> / <a href="javascript:hideshow(document.getElementById('PointNet'))">bibtex</a> / <a href="https://github.com/charlesq34/pointnet">code</a> / <a href="http://stanford.edu/~rqi/pointnet/">website</a> / <a href="https://www.youtube.com/watch?v=Cge-hot0Oc0">presentation video</a>
<p id="PointNet" style="font:18px; display: none">
@article{qi2017pointnet,
  title={PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},
  author={Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},
  journal={Proc. Computer Vision and Pattern Recognition (CVPR), IEEE},
  year={2017}
}
</tr>
</table>
</div>

<!-- SHAPE_COMPLETION -->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
<td width="25%" valign="center"><p><img src="papers/shape_completion.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="center"><p>
<div class="paper-title"><a href="https://arxiv.org/abs/1612.00101">Shape Completion using 3D-Encoder-Predictor CNNs and Shape Synthesis</a></div>
<div style="margin-bottom: 10px; padding: 0px">Angela Dai, <b>Charles R. Qi</b>, Matthias Niessner</div>
<div style="margin-bottom: 10px; padding: 0px"><i>CVPR 2017</i>, <font color="#d22d1d">Spotlight Presentation</font></div>
<a href="https://arxiv.org/pdf/1612.00101.pdf">paper</a> / <a href="javascript:hideshow(document.getElementById('ShapeCompletion'))">bibtex</a> / <a href="http://graphics.stanford.edu/projects/cnncomplete/">website (code & data available)</a>
<p id="ShapeCompletion" style="font:18px; display: none">
@article{dai2017complete,
  title={Shape Completion using 3D-Encoder-Predictor CNNs and Shape Synthesis},
  author={Dai, Angela and Qi, Charles Ruizhongtai and Nie{\ss}ner, Matthias},
  journal={Proc. Computer Vision and Pattern Recognition (CVPR), IEEE},
  year={2017}
}
</tr>
</table>
</div>

<!-- VOLUMETRIC CNN -->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
<td width="25%" valign="center"><p><img src="papers/volumetric_cnn.jpg" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="center"><p>
	<div class="paper-title"><a href="https://arxiv.org/abs/1604.03265">Volumetric and Multi-View CNNs for Object Classification on 3D Data</a></div>
	<div style="margin-bottom: 10px; padding: 0px"><b>Charles R. Qi</b><sup>*</sup>, Hao Su<sup>*</sup>, Matthias Niessner, Angela Dai, Mengyuan Yan, and Leonidas J. Guibas (<sup>*</sup>: equal contribution)</div>
<div style="margin-bottom: 10px; padding: 0px"><i>CVPR 2016</i>,<font color="#d22d1d">Spotlight Presentation</font></div>
<a href="https://arxiv.org/abs/1604.03265">paper</a> / <a href="javascript:hideshow(document.getElementById('VolumetricCNN'))">bibtex</a> / <a href="https://github.com/charlesq34/3dcnn.torch">code</a> / <a href="http://graphics.stanford.edu/projects/3dcnn/">website</a>  / <a href="papers/volumetric_cnn_cvpr16_supp.pdf">supp</a> / <a href="https://www.youtube.com/watch?v=bE7jzHJiQWw">presentation video</a>
<p id="VolumetricCNN" style="font:18px; display: none">
@inproceedings{qi2016volumetric,
  author = {Charles Ruizhongtai Qi and Hao Su and Matthias Nie{\ss}ner and 
    Angela Dai and Mengyuan Yan and Leonidas Guibas},
  title = {Volumetric and Multi-View CNNs for Object Classification on 3D Data},
  booktitle = {Proc. Computer Vision and Pattern Recognition (CVPR), IEEE},
  year = {2016}
}
</tr>
</table>
</div>

<!-- DEEP EMBEDDING -->
<div class="rounded-rectangle" style="margin-bottom:15px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
<td width="25%" valign="center"><p><img src="papers/joint_embedding.jpg" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="center"><p>
	<div class="paper-title"><a href="https://shapenet.cs.stanford.edu/projects/JointEmbedding/">Joint Embeddings of Shapes and Images via CNN Image Purification</a></div>
	<div style="margin-bottom: 10px; padding: 0px">Yangyan Li<sup>*</sup>, Hao Su<sup>*</sup>, <b>Charles R. Qi</b>, Noa Fish, Daniel Cohen-Or, and Leonidas J. Guibas (<sup>*</sup>: equal contribution)</div>
	<div style="margin-bottom: 10px; padding: 0px"><i>SIGGRAPH Asia 2015</i></div>
<a href="https://shapenet.cs.stanford.edu/projects/JointEmbedding/JointEmbedding.pdf">paper</a> / <a href="javascript:hideshow(document.getElementById('JointEmbeddingBib'))">bibtex</a> / <a href="http://shapenet.github.io/JointEmbedding/">code</a> / <a href="https://shapenet.cs.stanford.edu/projects/JointEmbedding/">website</a> / <a href="https://shapenet.cs.stanford.edu/shapenet_brain/app_joint_embedding/">live demo</a>
<p id="JointEmbeddingBib" style="font:18px; display: none">
@article{li2015jointembedding,
    Author = {Li, Yangyan and Su, Hao and Qi, Charles Ruizhongtai and Fish, Noa
        and Cohen-Or, Daniel and Guibas, Leonidas J.},
    Title = {Joint Embeddings of Shapes and Images via CNN Image Purification},
    Journal = {ACM Trans. Graph.},
    Year = {2015}
}
</tr>
</table>
</div>

<!-- VIEWPOINT -->
<div class="rounded-rectangle" style="margin-bottom:35px">
<table width="100%" style="line-height: 1.1; font-weight: 300; font-size: 16px; min-height: 160px;">
<tr>
<td width="25%" valign="center"><p><img src="papers/render_for_cnn.jpg" width="250" alt=""></p></td>
<td width="75%" valign="center">
<p>
<div class="paper-title"><a href="papers/render_for_cnn_iccv15.pdf">Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views</a></div>
<div style="margin-bottom: 10px; padding: 0px">Hao Su<sup>*</sup>, <b>Charles R. Qi</b><sup>*</sup>, Yangyan Li, Leonidas J. Guibas (<sup>*</sup>equal contribution)</div>
<div style="margin-bottom: 10px; padding: 0px"><i>ICCV 2015</i>, <font color="#d22d1d">Oral Presentation</font></div>
<a href="https://shapenet.cs.stanford.edu/projects/RenderForCNN/resources/RenderForCNN.pdf">paper</a> / <a href="javascript:hideshow(document.getElementById('RenderForCNNBib'))">bibtex</a> / <a href="https://github.com/ShapeNet/RenderForCNN">code</a> / <a href="https://shapenet.cs.stanford.edu/projects/RenderForCNN/">website</a> / <a href="http://videolectures.net/iccv2015_su_qi_viewpoint_estimation/">presentation video</a>
<p id="RenderForCNNBib" style="font:18px; display: none">
@InProceedings{Su_2015_ICCV,
    Title={Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views},
    Author={Su, Hao and Qi, Charles R. and Li, Yangyan and Guibas, Leonidas J.},
    Booktitle={The IEEE International Conference on Computer Vision (ICCV)},
    month = {December},
    Year= {2015}
}
</tr>
</table>
</div>


<a name="talks"></a>
<div class="mainsection" style="margin-top: 15px;">
<div style="font-weight: 500; font-size: 24px; margin-bottom: 10px">Talks</div>

<div>
<ul>
<li>
Invited Speaker. <i>3D Perception from Multi-X</i>. 2023.
<ul><li><a href="https://3dmv2023.github.io/">3DMV: Learning 3D with Multi-View Supervision, CVPR 2023 Workshop</a> <a href="https://drive.google.com/file/d/1prL0WYDh4sQwvAWgm7iP61GYRGEX7CeT/view">[video]</a></ul>
<li>
Invited Speaker. <i>Offboard Perception for Autonomous Driving</i>. 2021.
<ul>
<li>
<a href="https://sites.google.com/view/3d-dlad-v3-iv2021/schedule">3rd Workshop for Autnomous Driving, IV 2021</a> <a href="https://www.youtube.com/watch?v=hyTMyZgWcFY">[video]</a>
<li>
<a href="6th Workshop on Benchmarking Multi-Target Tracking">6th Workshop on Benchmarking Multi-Target Tracking, ICCV 2021</a> <a href="https://www.youtube.com/watch?v=jaVIlNKSYvI">[video]</a>
<li>
<a href="https://www.adp3.org/invited-talks#h.2nmnbhpzoevv">Workshop on Autonomous Driving: Perception, Prediction and Planning, CVPR 2021</a> <a href="https://youtu.be/COgEQuqTAug?t=14028">[video]</a>
</ul>

<li>
Invited Speaker and organizer. <i>Deep Learning on Point Cloud and Other 3D Forms</i>. <a href='http://3ddl.stanford.edu'>3D Deep Learning Tutorial</a> at CVPR 2017, Honolulu. <a href="https://www.youtube.com/watch?v=8CenT_4HWyY">[video]</a>
<li>
Guest Lecturer. <i>3D Object Detection: The History, Present and Future</i>. 2021. <a href="https://haosulab.github.io/ml-meets-geometry/WI22/index.html">CSE219: Machine Learning Meets Geometry, UC San Diego</a>. <a href="https://drive.google.com/file/d/10rJbk3VNRc2Uah5OQsRjFGnyILu7awQj/view?usp=sharing">[slides]</a>
<li>
Invited Speaker. <i>3D Object Recognition in Point Clouds</i>. 2020. <a href="http://vision.stanford.edu/">Stanford Vision and Learning Lab</a> <a href="https://drive.google.com/file/d/10rJbk3VNRc2Uah5OQsRjFGnyILu7awQj/view?usp=sharing">[slides]</a>

<li>
(&#20013;&#25991;) Invited Speaker. <i>The Development and Future of 3D Object Detection</i>. 2021. <a href="https://www.shenlanxueyuan.com/open/course/97">Shenlan Xueyuan</a>. <a href="https://www.bilibili.com/video/BV1wA411p7FZ?from=search&seid=5827865779459596600">[video]</a>
<li>
(&#20013;&#25991;) Invited Speaker. <i>Frustum PointNets for 3D Object Detection from RGB-D Data</i>. 2019. <a href="http://games-cn.org/games-webinar-2019117-82/">GAMES Webinar Series 82</a>. <a href="https://v.qq.com/x/page/i0834yyd0jg.html?">[video]</a> <a href="https://slides.games-cn.org/pdf/GAMES201982%E7%A5%81%E8%8A%AE%E4%B8%AD%E5%8F%B0.pdf">[slides]</a>
<li>
(&#20013;&#25991;) Invited Speaker. <i>Deep Hough Voting for 3D Object Detection in Point Clouds</i>. 2019. <a href="http://games-cn.org/games-webinar-20191205-121/">GAMES Webinar Series 121</a>. <a href="https://v.qq.com/x/page/v3031ehsz52.html">[video]</a> <a href="https://slides.games-cn.org/pdf/Games2019121CharlesQi.pdf">[slides]</a>
<li>
(&#20013;&#25991;) Invited Speaker. <i>Deep Learning on Point Clouds for 3D Scene Understanding</i>. 2018. <a href="https://www.techbeat.net/">Jiangmen TechBeat</a>. <a href="https://www.youtube.com/watch?v=Ew24Rac8eYE">[video]</a> <a href="https://www.techbeat.net/talk-info?id=254">[slides]</a>

<li>
Guest Lecturer. Spring 2017-18: <a href="http://cs233.stanford.edu">The Shape of Data: Geometric and Topological Data Analysis</a> at Stanford University.
<li>
Guest Lecturer. Spring 2016-17: <a href="http://graphics.stanford.edu/courses/cs468-17-spring/">Machine Learning for 3D Data</a> at Stanford University.
<ul>
</div>

<hr>
<div id="footer" style="font-size:12; text-align: center">Charles Ruizhongtai Qi <i>Last updated: Feb, 2024</i></div>
</body>
